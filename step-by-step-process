To connect to my gcp account from my remote Ubuntu VM:
create private and public key:
ssh-keygen -t rsa -P ""

install filezilla for sftp: 
sudo apt update
sudo apt install filezilla
filezilla <enter>
click edit --> Settings -> Select SFTP --> Add private key -> save --> ok
check top -> Host (sftp://<gcp ip address>) Username: <username>
(set No password  and no port)
click Quickconnect

Now in terminal:
ftp mohammad@<gcp_ip_address> <enter>

mget <filename>
sftp> mget TCS.csv
Fetching /home/mohammad/TCS.csv to TCS.csv
/home/mohammad/TCS.csv                                              100%  456KB 369.2KB/s   00:01    
sftp> mget ZEEL.csv
Fetching /home/mohammad/ZEEL.csv to ZEEL.csv
/home/mohammad/ZEEL.csv                                             100%  566KB 432.5KB/s   00:01    
sftp> mget TECHM.csv
Fetching /home/mohammad/TECHM.csv to TECHM.csv
/home/mohammad/TECHM.csv                                            100%  389KB 256.7KB/s   00:01   

----------------------------------------------------------------------------------

Data ingestion from Three different databases:


start-all.sh
jps
hdfs dfs -ls /user/
hdfs dfs -makdir /user/<hdfs directory>


1) Postgresql: SQOOP COMMAND

 sqoop import \
--connect 'jdbc:postgresql://localhost/postgres' \
--username 'postgres' \
--password '<password>' \
--m 1 \
--table <tablename> \
--target-dir </hdfs directory/>

2) MySql: SQOOP COMMAND

sqoop import \
--connect jdbc:mysql://localhost/<databasename> \
--username root \
--password <password> usually 'root' \
--m 1 \
--table <tablename> \
--target-dir </hdfs directory/>

3) SQL-Server: SQOOP COMMAND

sqoop import \
--connect 'jdbc:sqlserver://localhost;databasename=<databasename>' \
--username 'SA' -P \
--m 1 \
--table <tablename> \
--target-dir </hdfs directory/> 


See the output using below command:
hdfs dfs -ls </hdfs directory>
drwxr-xr-x   - <username> supergroup          0 2020-10-09 11:02 /user/used_cases/axis
drwxr-xr-x   - <username> supergroup          0 2020-10-09 11:09 /user/used_cases/bpcl
drwxr-xr-x   - <username> supergroup          0 2020-10-09 11:06 /user/used_cases/cipla
drwxr-xr-x   - <username> supergroup          0 2020-10-09 11:43 /user/used_cases/coalindia
drwxr-xr-x   - <username> supergroup          0 2020-10-09 11:42 /user/used_cases/grasim
drwxr-xr-x   - <username> supergroup          0 2020-10-09 11:36 /user/used_cases/hdfc
drwxr-xr-x   - <username> supergroup          0 2020-10-09 11:55 /user/used_cases/ntpc
drwxr-xr-x   - <username> supergroup          0 2020-10-09 11:51 /user/used_cases/sbin
drwxr-xr-x   - <username> supergroup          0 2020-10-09 11:57 /user/used_cases/wipro

------------------------------------------------------------------------

CREATING EXTERNAL TABLES IN HIVE:

1) WORKING WITH HDFS file:
   a) CREATE AN EXTENALE TABLE IN HIVE:
      > start-all.sh (always start all the daemons first)  
      > hive <enter> 
      hive > creat database used_case_external;
      hive > use used_case_external;
      hive > create external table <tablename> (schema) 
           > row format delimited
           > fields terminated by ',';
      (hdfs file:) 
      hive> load data inpath 'HDFS filepath' into table tablename;
      (LFS file:)
      hive> load data inpath 'LFS filepath' into table tablename;
2) WORKING WITH INTERNAL csv files:
      hive> create external table <tablename> (schema)
      hive> load data local inpath 'file1' into table <tablename>; 
----------------------------------------------------------------



SCHEMA:
create external table wipro_ext(date_val string , symbol string, series string, prev_close float, opev_val float, high_val float, low_val float, last_val float, close_val float, VWAP float, volume int, turnover float, trades string, deliverable_vol string, deliverable_pert string)


RESULTS FROM EXTERNAL DATA STORAGE (Dabases):

drwxr-xr-x   -  supergroup          0 2020-10-10 12:54 ../hive/warehouse/used_case.db/axis_ext
drwxr-xr-x   -  supergroup          0 2020-10-10 12:55 ../hive/warehouse/used_case.db/bpcl_ext
drwxr-xr-x   -  supergroup          0 2020-10-10 12:56 ../hive/warehouse/used_case.db/cipla_ext
drwxr-xr-x   -  supergroup          0 2020-10-10 12:57 ../hive/warehouse/used_case.db/coalindia_ext
drwxr-xr-x   -  supergroup          0 2020-10-10 12:58 ../hive/warehouse/used_case.db/grasim_ext
drwxr-xr-x   -  supergroup          0 2020-10-10 12:59 ../hive/warehouse/used_case.db/hdfc_ext
drwxr-xr-x   -  supergroup          0 2020-10-10 13:00 ../hive/warehouse/used_case.db/ntpc_ext
drwxr-xr-x   -  supergroup          0 2020-10-10 12:53 ../hive/warehouse/used_case.db/sbi_ext
drwxr-xr-x   - mohamm.. supergroup          0 2020-10-10 13:00 ../hive/warehouse/used_case.db/wipro_ext

hive> select * from wipro_ext limit 3;
2000-01-03	WIPRO	EQ	2522.4	2724.0	2724.2	2724.0	2724.2	2724.2	2724.17	1599	4.35594199E11	null	null	null
2000-01-04	WIPRO	EQ	2724.2	2942.15	2942.15	2942.15	2942.15	2942.15	2942.15	4392	1.29219232E12	null	null	null
2000-01-05	WIPRO	EQ	2942.15	2942.15	3177.55	2715.0	3000.0	2990.1	3063.86	132297	4.0533896E13	null	null	null

------------------------------------------------------------------------------------
IMPORT SFTP csv files into the external HIVE tables, FINALLY all the tables table:

Found 12 items
drwxr-xr-x   - m supergroup          0 2020-10-10 12:54 /hive/warehouse/used_case.db/axis_ext
drwxr-xr-x   - m supergroup          0 2020-10-10 12:55 /hive/warehouse/used_case.db/bpcl_ext
drwxr-xr-x   - m supergroup          0 2020-10-10 12:56 /hive/warehouse/used_case.db/cipla_ext
drwxr-xr-x   - m supergroup          0 2020-10-10 12:57 /hive/warehouse/used_case.db/coalindia_ext
drwxr-xr-x   - m supergroup          0 2020-10-10 12:58 /hive/warehouse/used_case.db/grasim_ext
drwxr-xr-x   - m supergroup          0 2020-10-10 12:59 /hive/warehouse/used_case.db/hdfc_ext
drwxr-xr-x   - m supergroup          0 2020-10-10 13:00 /hive/warehouse/used_case.db/ntpc_ext
drwxr-xr-x   - m supergroup          0 2020-10-10 12:53 /hive/warehouse/used_case.db/sbi_ext
drwxr-xr-x   - m supergroup          0 2020-10-10 16:09 /hive/warehouse/used_case.db/tcs__ext_csv
drwxr-xr-x   - m supergroup          0 2020-10-10 16:11 /hive/warehouse/used_case.db/techm__ext_csv
drwxr-xr-x   - m supergroup          0 2020-10-10 13:00 /hive/warehouse/used_case.db/wipro_ext
drwxr-xr-x   - m supergroup          0 2020-10-10 16:12 /hive/warehouse/used_case.db/zeel__ext_csv


List of external tables:
hive> show tables;
OK
axis_ext
bpcl_ext
cipla_ext
coalindia_ext
grasim_ext
hdfc_ext
ntpc_ext
sbi_ext
tcs__ext_csv
techm__ext_csv
wipro_ext
zeel__ext_csv

------------------------------------------------INTERNAL TABLES IN HIVE-------------------------------------
hive> create table axis_int(date_val string , symbol string, series string, prev_close float, opev_val float, high_val float, low_val float, last_val float, close_val float, VWAP float, volume int, turnover float, trades string, deliverable_vol string, deliverable_pert string);
OK
Time taken: 0.288 seconds

hive> insert overwrite table axis_int select * from used_case.axis_ext;

Repeat the same for other tables 11 tables.
Finally I got:
hive> show tables;
OK
axis_int
bpcl_int
cipla_int
coalindia_int
grasim_int
hdfc_int
ntpc_int
sbi_int
tcs__int_csv
techm__int_csv
wipro_int
zeel__int_csv

# Detailed Table Information	 	 
Database:           	usecase_internal    	 
Owner:              	mohammad            	 
CreateTime:         	Mon Oct 12 15:56:33 BST 2020	 
LastAccessTime:     	UNKNOWN             	 
Retention:          	0                   	 
Location:           	hdfs://localhost:9000/user/hive/warehouse/usecase_internal.db/axis_int	 
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	numFiles            	1                   
	numRows             	5163                
	rawDataSize         	569395              
	totalSize           	574558              
	transient_lastDdlTime	1602514656          



